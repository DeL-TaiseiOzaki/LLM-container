# ðŸš€ Simple LLM Docker Builder

ã‚·ãƒ³ãƒ—ãƒ«ã§å®Ÿç”¨çš„ãªLLMé–‹ç™ºç’°å¢ƒã‚’æ•°åˆ†ã§æ§‹ç¯‰ã™ã‚‹ãƒ„ãƒ¼ãƒ«

## âœ¨ ç‰¹å¾´

- **ã‚·ãƒ³ãƒ—ãƒ«**: è¤‡é›‘ãªè¨­å®šä¸è¦ã€YAMLãƒ•ã‚¡ã‚¤ãƒ«1ã¤ã§å®Œçµ
- **é«˜é€Ÿ**: uvãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒžãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§é«˜é€Ÿã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- **å®Ÿç”¨çš„**: å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã ã‘ã‚’é¸æŠžã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

## ðŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
```bash
git clone https://github.com/yourusername/LLM-container
cd llm-container
```

### 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†
```yaml
# config.yaml
cuda_version: "12.1"          # ã‚ãªãŸã®CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³
pytorch_version: "2.5.1"       # çœç•¥å¯èƒ½ï¼ˆè‡ªå‹•é¸æŠžï¼‰
transformers_version: "4.44.0" # ãŠå¥½ã¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³

packages:
  vllm: true                   # å¿…è¦ãªã‚‚ã®ã‚’true
  flash_attention2: true
  openai: true
  langchain: true
```

### 3. ãƒ“ãƒ«ãƒ‰ï¼†å®Ÿè¡Œ
```bash
# ã™ã¹ã¦è‡ªå‹•ã§å®Ÿè¡Œ
python build.py all

# ã¾ãŸã¯å€‹åˆ¥ã«å®Ÿè¡Œ
python build.py build  # Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ“ãƒ«ãƒ‰
python build.py run    # ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•
```

## ðŸ“¦ åˆ©ç”¨å¯èƒ½ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | èª¬æ˜Ž | ç”¨é€” |
|---------|------|-----|
| **vLLM** | é«˜é€ŸæŽ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ | æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ |
| **TRL** | å¼·åŒ–å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | RLHF/DPOè¨“ç·´ |
| **Unsloth** | é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | QLoRAè¨“ç·´ |
| **DeepSpeed** | åˆ†æ•£å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | å¤§è¦æ¨¡è¨“ç·´ |
| **Flash Attention 2** | é«˜é€Ÿã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ | ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡æ”¹å–„ |
| **OpenAI** | OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ | GPT-4/GPT-3.5é€£æº |
| **LiteLLM** | çµ±ä¸€APIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ | è¤‡æ•°LLMçµ±åˆ |
| **LangChain** | LLMã‚¢ãƒ—ãƒªé–‹ç™º | RAG/ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ |

## ðŸŽ¯ ã‚µãƒãƒ¼ãƒˆç’°å¢ƒ

### CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨PyTorchå¯¾å¿œè¡¨

| CUDA | æŽ¨å¥¨PyTorch | åˆ©ç”¨å¯èƒ½ãªPyTorch | æŽ¨å¥¨GPU |
|------|------------|----------------|---------|
| **11.8** | 2.5.1 | 2.7.0, 2.6.0, 2.5.1, 2.4.1, 2.3.1 | å¹…åºƒã„äº’æ›æ€§ï¼ˆT4, V100, A100ï¼‰ |
| **12.1** | 2.5.1 | 2.5.1, 2.4.1, 2.3.1 | A100, RTX 30xx |
| **12.4** | 2.6.0 | 2.6.0, 2.5.1, 2.4.1 | RTX 40xx |
| **12.6** | 2.7.0 | 2.7.0, 2.6.0 | æœ€æ–°GPU |
| **12.8** | 2.7.0 | 2.7.0 | H100, H200 |

### Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³
- 3.9, 3.10, 3.11, 3.12

## ðŸ’» ä½¿ç”¨ä¾‹

### ã‚³ãƒ³ãƒ†ãƒŠã«æŽ¥ç¶š
```bash
docker exec -it llm-dev bash
```

### Jupyter Labèµ·å‹•
```bash
docker exec -it llm-dev jupyter lab --ip 0.0.0.0 --allow-root
# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:8888 ã«ã‚¢ã‚¯ã‚»ã‚¹
```

### Claude Codeä½¿ç”¨
```bash
docker exec -it llm-dev claude
```

### Pythonã§ç¢ºèª
```python
import torch
import transformers

print(f"PyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Transformers: {transformers.__version__}")
```

## ðŸ—‚ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
simple-llm-docker/
â”œâ”€â”€ build.py           # ãƒ“ãƒ«ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ config.yaml        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ Dockerfile.j2  # Dockerfileãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
â”œâ”€â”€ requirements.txt   # Pythonä¾å­˜
â””â”€â”€ README.md         # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

## âš™ï¸ ã‚«ã‚¹ã‚¿ãƒžã‚¤ã‚º

### æœ€å°æ§‹æˆï¼ˆæŽ¨è«–ã®ã¿ï¼‰
```yaml
packages:
  vllm: true
  flash_attention2: true
  openai: true
```

### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ§‹æˆ
```yaml
packages:
  trl: true
  unsloth: true
  flash_attention2: true
  deepspeed: true
```

### ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯é–‹ç™º
```yaml
packages:
  vllm: true
  langchain: true
  litellm: true
  openai: true
  flash_attention2: true
```

## ðŸ› ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç¢ºèª
```bash
nvidia-smi  # ãƒ›ã‚¹ãƒˆãƒžã‚·ãƒ³ã§å®Ÿè¡Œ
```

### ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼
```bash
# Flash Attentionãƒ“ãƒ«ãƒ‰æ™‚ã®ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼å¯¾ç­–
# config.yamlã§ flash_attention2: false ã«è¨­å®š
```

### æ—¢å­˜ã‚³ãƒ³ãƒ†ãƒŠã®å‰Šé™¤
```bash
docker stop llm-dev
docker rm llm-dev
```

## ðŸ“ requirements.txt

```txt
pyyaml>=6.0
jinja2>=3.1.0
```

## ðŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License