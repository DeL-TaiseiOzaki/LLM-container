#!/usr/bin/env python3
"""
ä¾å­˜é–¢ä¿‚ã‚½ãƒ«ãƒãƒ¼ - ML/AIãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è‡ªå‹•ãƒãƒ¼ã‚¸ãƒ§ãƒ³è§£æ±º
"""

from __future__ import annotations
from typing import Dict, Any, List, Tuple, Optional, Set
from dataclasses import dataclass
from packaging.version import Version, InvalidVersion
import copy


@dataclass
class PackageConstraint:
    """ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®åˆ¶ç´„æ¡ä»¶"""
    name: str
    version_constraint: str  # ">=2.0.0", "==2.1.0", etc.
    platform_specific: bool = False
    cuda_suffix: Optional[str] = None
    reason: str = ""


@dataclass
class Resolution:
    """è§£æ±ºçµæœ"""
    success: bool
    packages: Dict[str, str]  # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å -> ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    install_commands: List[str]
    warnings: List[str]
    conflicts: List[str]


class DependencyResolver:
    """ä¾å­˜é–¢ä¿‚è§£æ±ºã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, compatibility_maps: Dict[str, Any]):
        self.compatibility_maps = compatibility_maps
        self.resolution_cache: Dict[str, Resolution] = {}
    
    def resolve_dependencies(
        self, 
        base_config: Dict[str, Any],
        strategy: str = "stability"  # "stability", "performance", "compatibility"
    ) -> Resolution:
        """
        ãƒ¡ã‚¤ãƒ³è§£æ±ºãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            base_config: åŸºæœ¬è¨­å®šï¼ˆCUDAã€Pythonã€GPUç­‰ï¼‰
            strategy: è§£æ±ºæˆ¦ç•¥
        
        Returns:
            Resolution: è§£æ±ºçµæœ
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache_key = self._generate_cache_key(base_config, strategy)
            if cache_key in self.resolution_cache:
                return self.resolution_cache[cache_key]
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: åŸºæœ¬åˆ¶ç´„ã®æŠ½å‡º
            constraints = self._extract_base_constraints(base_config)
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è‡ªå‹•è§£æ±º
            pytorch_resolution = self._resolve_pytorch_version(base_config, strategy)
            if not pytorch_resolution.success:
                return pytorch_resolution
            
            constraints.extend(pytorch_resolution.packages.items())
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: Flash Attentionã®è§£æ±º
            flash_attn_resolution = self._resolve_flash_attention(base_config, pytorch_resolution.packages)
            if flash_attn_resolution:
                constraints.extend(flash_attn_resolution.items())
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: Transformersç”Ÿæ…‹ç³»ã®è§£æ±º
            transformers_resolution = self._resolve_transformers_ecosystem(
                base_config, pytorch_resolution.packages
            )
            constraints.extend(transformers_resolution.items())
            
            # ã‚¹ãƒ†ãƒƒãƒ—5: ç«¶åˆãƒã‚§ãƒƒã‚¯ã¨æœ€çµ‚è§£æ±º
            final_resolution = self._finalize_resolution(constraints, base_config)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.resolution_cache[cache_key] = final_resolution
            
            return final_resolution
            
        except Exception as e:
            return Resolution(
                success=False,
                packages={},
                install_commands=[],
                warnings=[],
                conflicts=[f"è§£æ±ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"]
            )
    
    def _resolve_pytorch_version(
        self, 
        base_config: Dict[str, Any], 
        strategy: str
    ) -> Resolution:
        """PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è‡ªå‹•è§£æ±º"""
        
        cuda_version = base_config["base"]["cuda_version"]
        python_version = base_config["base"]["python_version"]
        pytorch_preference = base_config["deep_learning"]["pytorch"].get("version", "")
        
        # CUDAäº’æ›æ€§ãƒãƒƒãƒ—ã‹ã‚‰å€™è£œã‚’å–å¾—
        cuda_map = self.compatibility_maps["cuda_pytorch"]["cuda"]
        
        if cuda_version not in cuda_map:
            return Resolution(
                success=False,
                packages={},
                install_commands=[],
                warnings=[],
                conflicts=[f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³: {cuda_version}"]
            )
        
        compatible_pytorch_versions = cuda_map[cuda_version]["compatible_pytorch"]
        
        # æˆ¦ç•¥ã«åŸºã¥ã„ã¦ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’é¸æŠ
        if pytorch_preference and pytorch_preference in compatible_pytorch_versions:
            selected_pytorch = pytorch_preference
        else:
            selected_pytorch = self._select_pytorch_by_strategy(
                compatible_pytorch_versions, strategy
            )
        
        # Pythonäº’æ›æ€§ãƒã‚§ãƒƒã‚¯
        if not self._check_python_pytorch_compatibility(selected_pytorch, python_version):
            return Resolution(
                success=False,
                packages={},
                install_commands=[],
                warnings=[],
                conflicts=[f"PyTorch {selected_pytorch} ã¯Python {python_version} ã¨éäº’æ›"]
            )
        
        # CUDAã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä»•æ§˜ã‚’ç”Ÿæˆ
        cuda_suffix = self._get_cuda_suffix(cuda_version)
        pytorch_spec = f"torch=={selected_pytorch}+{cuda_suffix}"
        torchvision_spec = f"torchvision+{cuda_suffix}"
        torchaudio_spec = f"torchaudio+{cuda_suffix}"
        
        index_url = cuda_map[cuda_version]["index_url"]
        
        install_command = (
            f"uv pip install {pytorch_spec} {torchvision_spec} {torchaudio_spec} "
            f"--index-url {index_url}"
        )
        
        return Resolution(
            success=True,
            packages={
                "torch": f"{selected_pytorch}+{cuda_suffix}",
                "torchvision": f"auto+{cuda_suffix}",
                "torchaudio": f"auto+{cuda_suffix}"
            },
            install_commands=[install_command],
            warnings=[],
            conflicts=[]
        )
    
    def _resolve_flash_attention(
        self, 
        base_config: Dict[str, Any], 
        pytorch_packages: Dict[str, str]
    ) -> Optional[Dict[str, str]]:
        """Flash Attention ã®è§£æ±º"""
        
        # Flash AttentionãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
        if not base_config["deep_learning"]["attention"].get("flash_attention", False):
            return None
        
        gpu_arch = base_config["gpu"]["architecture"]
        cuda_version = base_config["base"]["cuda_version"]
        pytorch_version = pytorch_packages["torch"].split("+")[0]  # CUDAã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
        
        flash_map = self.compatibility_maps["flash_attention"]["flash_attention"]
        
        # GPU ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãæ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        gpu_recommendations = self.compatibility_maps["flash_attention"]["gpu_recommendations"]
        
        if gpu_arch not in gpu_recommendations:
            return {
                "flash_attention_note": f"GPU ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ {gpu_arch} ã¯Flash Attentionã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“"
            }
        
        recommended_version = gpu_recommendations[gpu_arch]["recommended_version"]
        
        if recommended_version is None:
            return {
                "flash_attention_alternative": "xformers"  # V100ç­‰ã®ä»£æ›¿æ¡ˆ
            }
        
        # äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
        if recommended_version in flash_map:
            flash_entry = flash_map[recommended_version]
            
            if (cuda_version in flash_entry["compatible_cuda"] and 
                pytorch_version in flash_entry["compatible_pytorch"]):
                
                return {
                    "flash_attn": recommended_version
                }
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä»–ã®äº’æ›ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ¢ã™
        for version, entry in flash_map.items():
            if (cuda_version in entry["compatible_cuda"] and 
                pytorch_version in entry["compatible_pytorch"]):
                return {
                    "flash_attn": version
                }
        
        return {
            "flash_attention_conflict": f"äº’æ›æ€§ã®ã‚ã‚‹Flash AttentionãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        }
    
    def _resolve_transformers_ecosystem(
        self, 
        base_config: Dict[str, Any], 
        pytorch_packages: Dict[str, str]
    ) -> Dict[str, str]:
        """Transformersç”Ÿæ…‹ç³»ã®è§£æ±º"""
        
        pytorch_version = pytorch_packages["torch"].split("+")[0]
        python_version = base_config["base"]["python_version"]
        
        ecosystem_map = self.compatibility_maps.get("transformers_ecosystem", {})
        
        result = {}
        
        # Transformersã®è§£æ±º
        if "transformers" in ecosystem_map:
            transformers_version = self._find_compatible_version(
                ecosystem_map["transformers"],
                pytorch_version,
                python_version
            )
            if transformers_version:
                result["transformers"] = transformers_version
        
        # TRLã®è§£æ±ºï¼ˆTransformersã«ä¾å­˜ï¼‰
        if "trl" in ecosystem_map and "transformers" in result:
            trl_version = self._find_compatible_trl_version(
                ecosystem_map["trl"],
                result["transformers"],
                pytorch_version
            )
            if trl_version:
                result["trl"] = trl_version
        
        # Accelerateã®è§£æ±º
        if "accelerate" in ecosystem_map:
            accelerate_version = self._find_compatible_version(
                ecosystem_map["accelerate"],
                pytorch_version,
                python_version
            )
            if accelerate_version:
                result["accelerate"] = accelerate_version
        
        # PEFTã®è§£æ±º
        if "peft" in ecosystem_map and "transformers" in result:
            peft_version = self._find_compatible_peft_version(
                ecosystem_map["peft"],
                result["transformers"],
                pytorch_version
            )
            if peft_version:
                result["peft"] = peft_version
        
        return result
    
    def _finalize_resolution(
        self, 
        constraints: List[Tuple[str, str]], 
        base_config: Dict[str, Any]
    ) -> Resolution:
        """æœ€çµ‚çš„ãªè§£æ±ºã¨ã‚³ãƒãƒ³ãƒ‰ç”Ÿæˆ"""
        
        packages = dict(constraints)
        install_commands = []
        warnings = []
        conflicts = []
        
        # ç«¶åˆãƒã‚§ãƒƒã‚¯
        ecosystem_conflicts = self.compatibility_maps.get("transformers_ecosystem", {}).get("conflicts", [])
        for conflict in ecosystem_conflicts:
            conflict_packages = conflict["packages"]
            if all(pkg in packages for pkg in conflict_packages):
                warnings.append(f"ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ç«¶åˆ: {', '.join(conflict_packages)} - {conflict['reason']}")
        
        # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰ã®ç”Ÿæˆ
        pytorch_cmd = self._generate_pytorch_install_command(packages, base_config)
        if pytorch_cmd:
            install_commands.append(pytorch_cmd)
        
        # ãã®ä»–ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
        other_packages = []
        for pkg, version in packages.items():
            if not pkg.startswith(("torch", "flash_attn")):
                if version and version != "auto":
                    other_packages.append(f"{pkg}=={version}")
                else:
                    other_packages.append(pkg)
        
        if other_packages:
            install_commands.append(f"uv pip install {' '.join(other_packages)}")
        
        # Flash Attentionã®ç‰¹åˆ¥å‡¦ç†
        if "flash_attn" in packages:
            flash_cmd = self._generate_flash_attention_command(packages["flash_attn"])
            install_commands.append(flash_cmd)
        
        return Resolution(
            success=True,
            packages=packages,
            install_commands=install_commands,
            warnings=warnings,
            conflicts=conflicts
        )
    
    # ========== ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ ==========
    
    def _generate_cache_key(self, config: Dict[str, Any], strategy: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ç”Ÿæˆ"""
        key_parts = [
            config["base"]["cuda_version"],
            config["base"]["python_version"],
            config["gpu"]["architecture"],
            config["deep_learning"]["pytorch"].get("version", "auto"),
            str(config["deep_learning"]["attention"].get("flash_attention", False)),
            strategy
        ]
        return "|".join(key_parts)
    
    def _extract_base_constraints(self, config: Dict[str, Any]) -> List[Tuple[str, str]]:
        """åŸºæœ¬åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        # æ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åˆ¶ç´„ã¨ã—ã¦è¿½åŠ 
        if config["deep_learning"]["pytorch"].get("version"):
            constraints.append(("torch", config["deep_learning"]["pytorch"]["version"]))
        
        return constraints
    
    def _select_pytorch_by_strategy(self, versions: List[str], strategy: str) -> str:
        """æˆ¦ç•¥ã«åŸºã¥ã„ãŸPyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³é¸æŠ"""
        try:
            sorted_versions = sorted(versions, key=lambda v: Version(v), reverse=True)
        except InvalidVersion:
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³è§£æã«å¤±æ•—ã—ãŸå ´åˆã¯æ–‡å­—åˆ—ã‚½ãƒ¼ãƒˆ
            sorted_versions = sorted(versions, reverse=True)
        
        if strategy == "performance" or strategy == "latest":
            return sorted_versions[0]  # æœ€æ–°
        elif strategy == "compatibility":
            return sorted_versions[-1]  # æœ€å¤
        else:  # "stability"
            # å®‰å®šç‰ˆã‚’é¸æŠï¼ˆ2.5.xç³»çµ±ã‚’å„ªå…ˆï¼‰
            for version in sorted_versions:
                if version.startswith("2.5"):
                    return version
            return sorted_versions[len(sorted_versions) // 2]  # ä¸­é–“
    
    def _get_cuda_suffix(self, cuda_version: str) -> str:
        """CUDAã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®ç”Ÿæˆ"""
        return f"cu{cuda_version.replace('.', '')}"
    
    def _check_python_pytorch_compatibility(self, pytorch_version: str, python_version: str) -> bool:
        """Python-PyTorchäº’æ›æ€§ãƒã‚§ãƒƒã‚¯"""
        python_map = self.compatibility_maps.get("pytorch_python", {}).get("pytorch", {})
        
        if pytorch_version in python_map:
            compatible_pythons = python_map[pytorch_version]["compatible_python"]
            return python_version in compatible_pythons
        
        return True  # ä¸æ˜ãªå ´åˆã¯äº’æ›ã¨ã¿ãªã™
    
    def _find_compatible_version(
        self, 
        package_map: Dict[str, Any], 
        pytorch_version: str, 
        python_version: str
    ) -> Optional[str]:
        """äº’æ›æ€§ã®ã‚ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ¤œç´¢"""
        
        for version, requirements in package_map.items():
            # PyTorchè¦ä»¶ãƒã‚§ãƒƒã‚¯
            if "requires_pytorch" in requirements:
                pytorch_req = requirements["requires_pytorch"]
                if not self._version_satisfies(pytorch_version, pytorch_req):
                    continue
            
            # Pythonè¦ä»¶ãƒã‚§ãƒƒã‚¯
            if "requires_python" in requirements:
                python_req = requirements["requires_python"]
                if not self._version_satisfies(python_version, python_req):
                    continue
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆdeprecatedã¯é¿ã‘ã‚‹ï¼‰
            if requirements.get("status") == "deprecated":
                continue
            
            return version
        
        return None
    
    def _find_compatible_trl_version(
        self, 
        trl_map: Dict[str, Any], 
        transformers_version: str, 
        pytorch_version: str
    ) -> Optional[str]:
        """TRLãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ¤œç´¢ï¼ˆTransformersè¦ä»¶ã‚’è€ƒæ…®ï¼‰"""
        
        for version, requirements in trl_map.items():
            # Transformersè¦ä»¶ãƒã‚§ãƒƒã‚¯
            if "requires_transformers" in requirements:
                transformers_req = requirements["requires_transformers"]
                if not self._version_satisfies(transformers_version, transformers_req):
                    continue
            
            # PyTorchè¦ä»¶ãƒã‚§ãƒƒã‚¯
            if "requires_pytorch" in requirements:
                pytorch_req = requirements["requires_pytorch"]
                if not self._version_satisfies(pytorch_version, pytorch_req):
                    continue
            
            return version
        
        return None
    
    def _find_compatible_peft_version(
        self, 
        peft_map: Dict[str, Any], 
        transformers_version: str, 
        pytorch_version: str
    ) -> Optional[str]:
        """PEFTãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ¤œç´¢ï¼ˆTransformersè¦ä»¶ã‚’è€ƒæ…®ï¼‰"""
        
        for version, requirements in peft_map.items():
            # Transformersè¦ä»¶ãƒã‚§ãƒƒã‚¯
            if "requires_transformers" in requirements:
                transformers_req = requirements["requires_transformers"]
                if not self._version_satisfies(transformers_version, transformers_req):
                    continue
            
            # PyTorchè¦ä»¶ãƒã‚§ãƒƒã‚¯
            if "requires_pytorch" in requirements:
                pytorch_req = requirements["requires_pytorch"]
                if not self._version_satisfies(pytorch_version, pytorch_req):
                    continue
            
            return version
        
        return None
    
    def _version_satisfies(self, version: str, requirement: str) -> bool:
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¦ä»¶ã®æº€è¶³ãƒã‚§ãƒƒã‚¯"""
        try:
            actual_version = Version(version)
            
            # ç°¡å˜ãªè¦ä»¶è§£æï¼ˆ">=" ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼‰
            if requirement.startswith(">="):
                required_version = Version(requirement[2:])
                return actual_version >= required_version
            elif requirement.startswith("=="):
                required_version = Version(requirement[2:])
                return actual_version == required_version
            elif requirement.startswith(">"):
                required_version = Version(requirement[1:])
                return actual_version > required_version
            else:
                return True  # ä¸æ˜ãªå ´åˆã¯æº€è¶³ã¨ã¿ãªã™
                
        except InvalidVersion:
            return True
    
    def _generate_pytorch_install_command(self, packages: Dict[str, str], config: Dict[str, Any]) -> Optional[str]:
        """PyTorchã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰ã®ç”Ÿæˆ"""
        
        cuda_version = config["base"]["cuda_version"]
        
        pytorch_packages = []
        for pkg_name in ["torch", "torchvision", "torchaudio"]:
            if pkg_name in packages:
                pytorch_packages.append(packages[pkg_name])
        
        if not pytorch_packages:
            return None
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹URLã®å–å¾—
        cuda_map = self.compatibility_maps["cuda_pytorch"]["cuda"]
        index_url = cuda_map[cuda_version]["index_url"]
        
        return f"uv pip install {' '.join(pytorch_packages)} --index-url {index_url}"
    
    def _generate_flash_attention_command(self, version: str) -> str:
        """Flash Attentionã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰ã®ç”Ÿæˆ"""
        return f"MAX_JOBS=4 uv pip install flash-attn=={version} --no-build-isolation"


# ========== ä¾¿åˆ©é–¢æ•° ==========

def auto_resolve_config(partial_config: Dict[str, Any], compatibility_maps: Dict[str, Any]) -> Dict[str, Any]:
    """éƒ¨åˆ†è¨­å®šã‹ã‚‰å®Œå…¨è¨­å®šã‚’è‡ªå‹•ç”Ÿæˆ"""
    
    resolver = DependencyResolver(compatibility_maps)
    resolution = resolver.resolve_dependencies(partial_config)
    
    if not resolution.success:
        raise ValueError(f"ä¾å­˜é–¢ä¿‚è§£æ±ºã«å¤±æ•—: {', '.join(resolution.conflicts)}")
    
    # å…ƒã®è¨­å®šã«è§£æ±ºçµæœã‚’ãƒãƒ¼ã‚¸
    result_config = copy.deepcopy(partial_config)
    
    # PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ›´æ–°
    if "torch" in resolution.packages:
        torch_version = resolution.packages["torch"].split("+")[0]
        result_config["deep_learning"]["pytorch"]["version"] = torch_version
    
    # Flash Attentionãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ›´æ–°
    if "flash_attn" in resolution.packages:
        result_config["deep_learning"]["attention"]["flash_attention_version"] = resolution.packages["flash_attn"]
    
    return result_config


def print_resolution_report(resolution: Resolution) -> None:
    """è§£æ±ºçµæœã®ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›"""
    
    print("ğŸ”§ ä¾å­˜é–¢ä¿‚è§£æ±ºçµæœ")
    print("=" * 50)
    
    if resolution.success:
        print("âœ… è§£æ±ºæˆåŠŸ")
        
        print("\nğŸ“¦ è§£æ±ºã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:")
        for package, version in resolution.packages.items():
            print(f"  {package}: {version}")
        
        print("\nğŸ’» ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰:")
        for cmd in resolution.install_commands:
            print(f"  {cmd}")
        
        if resolution.warnings:
            print("\nâš ï¸  è­¦å‘Š:")
            for warning in resolution.warnings:
                print(f"  - {warning}")
    
    else:
        print("âŒ è§£æ±ºå¤±æ•—")
        
        if resolution.conflicts:
            print("\nğŸ”´ ç«¶åˆ:")
            for conflict in resolution.conflicts:
                print(f"  - {conflict}")
    
    print("=" * 50)