# 自動生成されたDockerfile - {{ timestamp }}
FROM {{ base.image }}:{{ base.tag }}

# 環境変数設定
ENV BNB_CUDA_VERSION={{ cuda_short }}
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# OSパッケージのインストール
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    ca-certificates \
    git \
    build-essential \
    cmake \
    ninja-build \
    wget \
{% if claude_code.install %}
    # Node.jsとnpmのインストール (Claude Code用)
    && curl -fsSL https://deb.nodesource.com/setup_20.x | bash - \
    && apt-get install -y nodejs \
{% endif %}
    && rm -rf /var/lib/apt/lists/*

# NVIDIA ベースイメージに含まれる既存のPyTorchをアンインストール（必要に応じて）
RUN pip uninstall -y torch torchvision torchaudio

# 指定したCUDAバージョンに合わせたPyTorchをインストール
{{ pytorch_install_command }}

# PyTorchのバージョン確認
RUN python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')"

# bitsandbytes のビルド（CUDA {{ base.cuda_version }}対応）
RUN git clone https://github.com/bitsandbytes-foundation/bitsandbytes.git && \
    cd bitsandbytes && \
    export BNB_CUDA_VERSION={{ cuda_short }} && \
    cmake -DCOMPUTE_BACKEND=cuda -S . && \
    make && \
    pip install -e . && \
    cd .. && \
    python -c "import bitsandbytes; print(f'bitsandbytes installed successfully: {bitsandbytes.__version__}')" || echo "Warning: bitsandbytes import test failed"

{% if deep_learning.attention.xformers %}
# xformers のインストール
RUN pip install -U "xformers{% if deep_learning.attention.xformers_version != 'latest' %}=={{ deep_learning.attention.xformers_version }}{% endif %}" --index-url https://download.pytorch.org/whl/cu{{ cuda_short }} --no-deps && \
    python -c "import xformers; print('xformers installed successfully')" || echo "Warning: xformers import test failed"
{% endif %}

{% if deep_learning.attention.flash_attention %}
# Flash Attention のインストール
RUN pip install ninja && \
    pip install "flash-attn=={{ deep_learning.attention.flash_attention_version }}" --no-build-isolation && \
    python -c "import flash_attn; print(f'Flash Attention installed successfully: {flash_attn.__version__}')" || echo "Warning: Flash Attention import test failed"
{% endif %}

{% if deep_learning.attention.triton %}
# Triton のインストール
RUN pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly && \
    python -c "import triton; print('Triton installed successfully')" || echo "Warning: Triton import test failed"
{% endif %}

# 必要な依存関係のインストール
RUN pip install --no-cache-dir \
{% for lib in libraries_flat %}
{% if lib.install is defined and lib.install %}
    {{ lib.name }}{% if lib.extra is defined %}{{ lib.extra }}{% endif %} \
{% elif lib.source is defined %}
    "{{ lib.name }} @ {{ lib.source }}" \
{% else %}
    {{ lib.name }}{% if lib.version is defined and lib.version != "latest" %}{{ lib.version }}{% endif %}{% if lib.extra is defined %}{{ lib.extra }}{% endif %} \
{% endif %}
{% endfor %}
    && echo "Python libraries installed successfully"

{% if claude_code.install %}
# Claude Codeのインストール
RUN npm install -g @anthropic-ai/claude-code{% if claude_code.version != "latest" %}@{{ claude_code.version }}{% endif %} && \
    echo "Claude Code installed successfully" || echo "Warning: Claude Code installation failed"
{% endif %}

{% if gpu.count > 1 or gpu.multi_node %}
# 分散処理環境のセットアップ
{% if distributed.enable %}
ENV MASTER_ADDR=localhost
ENV MASTER_PORT=29500
ENV WORLD_SIZE={{ gpu.count }}
{% if gpu.multi_node %}
ENV NODE_RANK=0
ENV NUM_NODES={{ gpu.nodes }}
{% endif %}
{% endif %}
{% endif %}

# 環境変数の設定
{% for env in env_vars %}
ENV {{ env.name }}={{ env.value }}
{% endfor %}

# 作業ディレクトリを設定
WORKDIR {{ workspace.directory }}

# 最終確認
RUN echo "Final environment check:" && \
    python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA available: {torch.cuda.is_available()}, CUDA version: {torch.version.cuda}, GPU count: {torch.cuda.device_count()}')" && \
{% if deep_learning.attention.flash_attention %}
    python -c "import flash_attn; print(f'Flash Attention: {flash_attn.__version__}')" || echo "Flash Attention not verified" && \
{% endif %}
{% if 'transformers' in libraries_flat|map(attribute='name') %}
    python -c "import transformers; print(f'Transformers: {transformers.__version__}')" || echo "Transformers not verified" && \
{% endif %}
{% if 'vllm' in libraries_flat|map(attribute='name') %}
    python -c "import vllm; print(f'vLLM: {vllm.__version__}')" || echo "vLLM not verified" && \
{% endif %}
{% if 'lightllm' in libraries_flat|map(attribute='name') %}
    python -c "import lightllm; print('LightLLM installed')" || echo "LightLLM not verified" && \
{% endif %}
{% if 'torchtune' in libraries_flat|map(attribute='name') %}
    python -c "import torchtune; print('TorchTune installed')" || echo "TorchTune not verified" && \
{% endif %}
{% if 'unsloth' in libraries_flat|map(attribute='name') %}
    python -c "import unsloth; print(f'Unsloth: {unsloth.__version__}')" || echo "Unsloth not verified" && \
{% endif %}
    echo "Installation complete!"

{% if gpu.count > 1 %}
# マルチGPU環境のテスト
RUN echo "Testing Multi-GPU setup:" && \
    python -c "import torch; [print(f'GPU {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())]" || echo "Multi-GPU setup not verified"
{% endif %}

# Blackwellアーキテクチャのサポート確認（CUDA 12.6以上の場合）
{% if base.cuda_version|float >= 12.6 and gpu.architecture == "blackwell" %}
RUN echo "Blackwell architecture support check:" && \
    python -c "import torch; print(f'CUDA capabilities: {[torch.cuda.get_device_capability(i) for i in range(torch.cuda.device_count())]}')" || echo "Architecture check failed"
{% endif %}