# Flash Attentionの互換性マップ
# 情報源: https://github.com/Dao-AILab/flash-attention
flash_attention:
  "2.3.3":
    compatible_cuda: ["11.8", "12.0", "12.1"]
    compatible_pytorch: ["2.0.0", "2.1.0", "2.2.0"]
  "2.3.0":
    compatible_cuda: ["11.8", "12.0"]
    compatible_pytorch: ["2.0.0", "2.1.0"]
  "2.0.0":
    compatible_cuda: ["11.6", "11.7", "11.8"]
    compatible_pytorch: ["1.12.0", "1.13.0", "2.0.0"]