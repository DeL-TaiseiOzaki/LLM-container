# Flash Attentionの互換性マップ（2025年6月版）
# 情報源: https://github.com/Dao-AILab/flash-attention + 調査結果
flash_attention:
  "2.8.0.post2":
    compatible_cuda: ["12.0", "12.1", "12.4", "12.6", "12.8"]
    compatible_pytorch: ["2.4.0", "2.4.1", "2.5.0", "2.5.1", "2.6.0", "2.7.0"]
    min_compute_capability: "8.0"  # Ampere以降
    supported_architectures: ["ampere", "ada", "hopper"]
    gpu_models: ["A100", "A6000", "RTX 3090", "RTX 4090", "H100", "H800"]
    memory_requirement_gb: 5.3  # ビルド時
    prebuilt_available: true
    status: "stable"
  "2.7.4.post1":
    compatible_cuda: ["11.4", "11.8", "12.0", "12.1", "12.4"]
    compatible_pytorch: ["2.0.0", "2.1.0", "2.2.0", "2.3.0", "2.4.0"]
    min_compute_capability: "7.5"
    supported_architectures: ["turing", "ampere", "ada"]
    gpu_models: ["T4", "RTX 20xx", "RTX 30xx", "A100", "RTX 4090"]
    prebuilt_available: true
    status: "legacy_stable"
  "2.3.3":
    compatible_cuda: ["11.8", "12.0", "12.1"]
    compatible_pytorch: ["2.0.0", "2.1.0", "2.2.0"]
    min_compute_capability: "7.5"
    supported_architectures: ["turing", "ampere"]
    prebuilt_available: false
    status: "deprecated"

# GPU アーキテクチャ別の推奨バージョン
gpu_recommendations:
  "hopper":  # H100, H800
    recommended_version: "2.8.0.post2"
    min_cuda: "12.1"
    optimal_cuda: "12.8"
  "ada":  # RTX 40xx
    recommended_version: "2.8.0.post2"
    min_cuda: "12.0"
    optimal_cuda: "12.4"
  "ampere":  # A100, RTX 30xx
    recommended_version: "2.8.0.post2"
    min_cuda: "11.8"
    optimal_cuda: "12.1"
  "turing":  # T4, RTX 20xx
    recommended_version: "2.7.4.post1"
    min_cuda: "11.8"
    optimal_cuda: "11.8"
  "volta":  # V100 (Flash Attention not supported)
    recommended_version: null
    alternative: "xformers"

# インストール設定
installation:
  prebuilt_url: "https://pypi.org/simple/flash-attn/"
  build_args:
    MAX_JOBS: 4
    TORCH_CUDA_ARCH_LIST: "auto"
  env_vars:
    FORCE_CUDA: "1"
    TORCH_COMPILE_DISABLE: "True"  # ビルド時の問題回避