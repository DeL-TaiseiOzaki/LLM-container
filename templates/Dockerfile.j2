# 自動生成されたDockerfile - {{ timestamp }}
FROM {{ base.image }}:{{ base.tag }}

# ────────────── 基本 ENV ──────────────
ENV BNB_CUDA_VERSION={{ base.cuda_version | replace(".", "") }}
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# ────────────── uv設定 ──────────────
ENV UV_SYSTEM_PYTHON=1
ENV UV_CACHE_DIR=/opt/uv-cache
ENV UV_NO_PROGRESS=1

# ────────────── OS パッケージ ──────────────
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl ca-certificates git build-essential cmake ninja-build wget \
    && rm -rf /var/lib/apt/lists/*

# ────────────── uv インストール ──────────────
RUN curl -LsSf https://astral.sh/uv/install.sh | sh && \
    mv /root/.cargo/bin/uv /usr/local/bin/

# uvのキャッシュディレクトリを作成
RUN mkdir -p ${UV_CACHE_DIR}

{% if claude_code.install %}
# ────────────── Node.js (uv経由でnodeenvを使用) ──────────────
RUN uv pip install nodeenv && \
    nodeenv -p --node={{ claude_code.nodejs_version|default('lts') }} && \
    npm install -g @anthropic-ai/claude-code
{% endif %}

# ────────────── PyTorch ──────────────
{% if deep_learning.pytorch.source == "pip" %}
# 既存のPyTorchを削除
RUN uv pip uninstall torch torchvision torchaudio || true

# PyTorchをuvでインストール
{% set cuda_index = "cu" + cuda_short %}
RUN uv pip install \
    torch=={{ deep_learning.pytorch.version }} \
    torchvision \
    torchaudio \
    --index-url https://download.pytorch.org/whl/{{ cuda_index }}
{% endif %}

# ────────────── bitsandbytes（ソースからビルド） ──────────────
# bitsandbytesは特殊なビルドが必要なので従来通り
RUN git clone --depth 1 https://github.com/bitsandbytes-foundation/bitsandbytes.git && \
    cd bitsandbytes && \
    export BNB_CUDA_VERSION={{ base.cuda_version | replace(".", "") }} && \
    cmake -DCOMPUTE_BACKEND=cuda -S . && \
    make -j$(nproc) && \
    pip install -e . && \
    cd .. && rm -rf bitsandbytes

{% if deep_learning.attention.xformers %}
# ────────────── xformers ──────────────
RUN uv pip install \
    xformers{% if deep_learning.attention.xformers_version %}=={{ deep_learning.attention.xformers_version }}{% endif %} \
    --index-url https://download.pytorch.org/whl/{{ cuda_short }}
{% endif %}

{% if deep_learning.attention.flash_attention %}
# ────────────── Flash-Attention ──────────────
# ビルド依存関係をuvでインストール
RUN uv pip install ninja packaging wheel setuptools

# Flash Attentionのビルドとインストール
RUN TORCH_CUDA_ARCH_LIST="{{ gpu_arch_list|default('8.0;8.6;9.0') }}" \
    MAX_JOBS=$(nproc) \
    uv pip install flash-attn=={{ deep_learning.attention.flash_attention_version }} \
    --no-build-isolation
{% endif %}

{% if deep_learning.attention.triton %}
# ────────────── Triton ──────────────
RUN uv pip install \
    --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ \
    triton-nightly
{% endif %}

# ────────────── ライブラリの一括インストール ──────────────
{% if libraries_flat %}
# requirements.txtを生成
RUN cat > /tmp/requirements.txt << 'EOF'
{%- for lib in libraries_flat %}
{%- if lib.source is defined %}
{{ lib.source }}
{%- elif lib.version is defined %}
{{ lib.name }}{{ lib.version }}{% if lib.extra is defined %}[{{ lib.extra }}]{% endif %}
{%- else %}
{{ lib.name }}{% if lib.extra is defined %}[{{ lib.extra }}]{% endif %}
{%- endif %}
{% endfor %}
EOF

# uvで一括インストール（並列処理で高速化）
RUN uv pip install -r /tmp/requirements.txt && \
    rm /tmp/requirements.txt
{% endif %}

# ────────────── uvキャッシュの最適化 ──────────────
# 不要なキャッシュを削除してイメージサイズを削減
RUN uv cache clean

# ────────────── 分散設定 ──────────────
ENV MASTER_ADDR=localhost
ENV MASTER_PORT=29500
ENV WORLD_SIZE={{ gpu.count }}

# ────────────── カスタム ENV ──────────────
{% for env_var in env_vars %}
ENV {{ env_var.name }}={{ env_var.value }}
{% endfor %}

WORKDIR {{ workspace.directory }}

# ────────────── 動作チェック ──────────────
RUN python -c "
import sys
import importlib
import torch

print('=== System Info ===')
print(f'Python: {sys.version}')
print(f'PyTorch: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'GPU count: {torch.cuda.device_count()}')

if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
        print(f'  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB')
else:
    print('WARNING: No GPU detected!')

print('\n=== Installed Packages ===')
packages = ['transformers', 'accelerate', 'peft', 'flash_attn', 'xformers', 'vllm', 'trl']
for pkg in packages:
    try:
        mod = importlib.import_module(pkg.replace('-', '_'))
        version = getattr(mod, '__version__', 'unknown')
        print(f'{pkg}: {version}')
    except ImportError:
        pass
"

# パッケージリストを記録
RUN uv pip list > /installed-packages.txt

# ヘルスチェック用スクリプト
RUN echo '#!/bin/bash\npython -c "import torch; assert torch.cuda.is_available()"' > /healthcheck.sh && \
    chmod +x /healthcheck.sh

HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD /healthcheck.sh || exit 1