# 自動生成されたDockerfile - {{ timestamp }}
FROM {{ base.image }}:{{ base.tag }}

# ────────────── 基本 ENV ──────────────
ENV BNB_CUDA_VERSION={{ cuda_short[2:] }}
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# ────────────── OS パッケージ ──────────────
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl ca-certificates git build-essential cmake ninja-build wget \
{% if claude_code.install %}
    # Node.js と npm（Claude Code 用）
    && curl -fsSL https://deb.nodesource.com/setup_20.x | bash - \
    && apt-get install -y nodejs \
{% endif %}
    && rm -rf /var/lib/apt/lists/*

# ────────────── PyTorch ──────────────
RUN pip uninstall -y torch torchvision torchaudio || true
RUN {{ pytorch_install_command }}

# ────────────── bitsandbytes（手動ビルド） ──────────────
RUN git clone --depth 1 https://github.com/bitsandbytes-foundation/bitsandbytes.git && \
    cd bitsandbytes && \
    export BNB_CUDA_VERSION={{ cuda_short[2:] }} && \
    cmake -DCOMPUTE_BACKEND=cuda -S . && \
    make -j$(nproc) && \
    pip install -e . && \
    cd .. && rm -rf bitsandbytes

{% if deep_learning.attention.xformers %}
# ────────────── xformers ──────────────
RUN pip install -U "xformers{% if deep_learning.attention.xformers_version != 'latest' %}=={{ deep_learning.attention.xformers_version }}{% endif %}" \
    --index-url https://download.pytorch.org/whl/{{ cuda_short }} --no-deps
{% endif %}

{% if deep_learning.attention.flash_attention %}
# ────────────── Flash-Attention ──────────────
RUN pip install ninja && \
    pip install "flash-attn=={{ deep_learning.attention.flash_attention_version }}" --no-build-isolation
{% endif %}

{% if deep_learning.attention.triton %}
# ────────────── Triton ──────────────
RUN pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly
{% endif %}

# ────────────── 追加 Python ライブラリ ──────────────
RUN pip install --no-cache-dir \
{% for lib in libraries_flat %}
{% if lib.install is defined and lib.install %}
    {{ lib.name }}{% if lib.extra is defined %}{{ lib.extra }}{% endif %} \
{% elif lib.source is defined %}
    "{{ lib.source }}" \
{% else %}
    {{ lib.name }}{% if lib.version is defined and lib.version != "latest" %}{{ lib.version }}{% endif %}{% if lib.extra is defined %}{{ lib.extra }}{% endif %} \
{% endif %}
{% endfor %}
    && echo "Python libraries installed successfully"

{% if claude_code.install %}
# ────────────── Claude Code ──────────────
RUN npm install -g @anthropic-ai/claude-code{% if claude_code.version != "latest" %}@{{ claude_code.version }}{% endif %}
{% endif %}

# ────────────── 分散設定（必要に応じて） ──────────────
{% if distributed.enable %}
ENV MASTER_ADDR=localhost
ENV MASTER_PORT=29500
ENV WORLD_SIZE={{ gpu.count }}
{% if gpu.multi_node %}
ENV NODE_RANK=0
ENV NUM_NODES={{ gpu.nodes }}
{% endif %}
{% endif %}

# ────────────── カスタム ENV ──────────────
{% for env in env_vars %}
ENV {{ env.name }}={{ env.value }}
{% endfor %}

WORKDIR {{ workspace.directory }}

# ────────────── 動作チェック ──────────────
RUN python - <<'PY'
import torch, importlib
print("CUDA available:", torch.cuda.is_available())
print("GPU count:", torch.cuda.device_count())
for mod in ("flash_attn","transformers","vllm","lightllm","torchtune","unsloth"):
    try:
        m=importlib.import_module(mod)
        print(mod, getattr(m,"__version__", "imported"))
    except ImportError:
        pass
PY

{% if gpu.count|int > 1 %}
RUN python - <<'PY'
import torch
[print(f"GPU {i}: {torch.cuda.get_device_name(i)}") for i in range(torch.cuda.device_count())]
PY
{% endif %}

{% if base.cuda_version|float >= 12.6 and gpu.architecture == "blackwell" %}
RUN python - <<'PY'
import torch
print("Device capabilities:", [torch.cuda.get_device_capability(i) for i in range(torch.cuda.device_count())])
PY
{% endif %}
